{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-12 11:36:37--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170498071 (163M) [application/x-gzip]\n",
      "Saving to: ‘data/input_data.tar.gz’\n",
      "\n",
      "data/input_data.tar 100%[===================>] 162.60M  52.7MB/s    in 3.2s    \n",
      "\n",
      "2020-01-12 11:36:41 (50.6 MB/s) - ‘data/input_data.tar.gz’ saved [170498071/170498071]\n",
      "\n",
      "cifar-10-batches-py/\n",
      "cifar-10-batches-py/data_batch_4\n",
      "cifar-10-batches-py/readme.html\n",
      "cifar-10-batches-py/test_batch\n",
      "cifar-10-batches-py/data_batch_3\n",
      "cifar-10-batches-py/batches.meta\n",
      "cifar-10-batches-py/data_batch_2\n",
      "cifar-10-batches-py/data_batch_5\n",
      "cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "!bash ../download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fzk2TbGbr3BC"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObiNGOTB96rI"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxAVOb9pVVRZ"
   },
   "source": [
    "## Preprocessing Modules\n",
    "\n",
    "* DataLoader: Class that can be run to load the training data for Cifar-10 and runs preprocessing + data augmentation\n",
    "\n",
    "\n",
    "        argument\n",
    "            input_dir - directory of your inputs\n",
    "            data_augmentation_functions - a list of functions that can be applied to an individual image\n",
    "                                          for augmentation. if left blank, images won't be augmented\n",
    "            peprocessing_functions - a list of functions that can be applied to an entire\n",
    "                                     np array of images that can be used for dataprep and will\n",
    "                                     be run on both the TRAIN and TEST SET, if left blank\n",
    "                                     images won't be augmented\n",
    "        returns\n",
    "            torch datasets for:\n",
    "                training data\n",
    "                test data\n",
    "                label encodings\n",
    "        usage:\n",
    "            data_loader = DataLoader(input_dir, data_augmentation_functions, preprocessing_functions)\n",
    "            train_data, test_data, label_encodings = data_loader.execute()\n",
    "\n",
    "* Transformation Functions: a series of simple data transformation functions that I implemented\n",
    "  * In this case I include:\n",
    "    * min_max_scaling\n",
    "    * random rotation\n",
    "    * horizontal flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "vg428Fya_QUM",
    "outputId": "3d6386f2-7acc-4bcf-8bb0-e39b71c3adde"
   },
   "outputs": [],
   "source": [
    "from preprocessing.DataLoader import DataLoader \n",
    "from preprocessing.TransformationFunctions import min_max_scaling, rotate_random, flip_horizontal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgD3TZp3nT5B"
   },
   "source": [
    "## Model Training Functions\n",
    "* I'm too lazy to explain this rn but I'll add docs later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "gvHY0dJhmvr-",
    "outputId": "a68ecaae-60a5-4c75-9cd3-45ca2b901ffd"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_stats(losses):\n",
    "    \"\"\"\n",
    "        prints out the train vs validation loss, error rate, and accuracy over time\n",
    "        --- this might only print out properly on google collab\n",
    "        argument - \n",
    "            losses: pd dataframe of evaluation stats at each epoch\n",
    "        returns\n",
    "            none\n",
    "    \"\"\"\n",
    "    losses[['train_loss', 'validation_loss']].plot(kind='line', title='train vs validation loss by epoch')\n",
    "\n",
    "    #it can be more interpretable to look at error rate, as opposed to accuracy to compare with the loss values \n",
    "    losses['error rate'] = 1 - losses['accuracy']\n",
    "   \n",
    "    losses[['error rate']].plot(kind='line', title='error rate by epoch')\n",
    "    losses[['accuracy']].plot(kind='line', title='accuracy by epoch')\n",
    "\n",
    "def train_model(net, \n",
    "                train_data, \n",
    "                test_data,\n",
    "                batch_size = 128,\n",
    "                n_epochs = 20, \n",
    "                learning_rate = .001, \n",
    "                weight_decay = None,\n",
    "                optim_method = optim.SGD,\n",
    "                cuda = False\n",
    "                ):\n",
    "    \"\"\"\n",
    "        trains a pytorch model and returns a dataframe of the training_loss,\n",
    "        validation_loss, and accuracy at each epoch\n",
    "\n",
    "        arguments - \n",
    "            net: pytorch nn object\n",
    "            train_data: training data\n",
    "            test_data: test data\n",
    "            batch_size: batch size \n",
    "            n_epochs: number of epochs\n",
    "            learning_rate: learning rate for optimizer\n",
    "            optim_method: optimizer function \n",
    "        returns\n",
    "            training loss: list of training loss at each epoch\n",
    "            validation loss: list of validation loss at each epoch\n",
    "            validation accuracy: list of accuracy on the validation set \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    training_loss, validation_loss, accuracy_list = [], [], []\n",
    "    \n",
    "    train_loader = get_data_loader(train_data, batch_size)\n",
    "    test_loader = get_data_loader(test_data, batch_size)\n",
    "\n",
    "    n_batches = len(train_loader)\n",
    "    loss, optimizer = get_loss_and_optimizer(net, learning_rate, optim_method, weight_decay)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        total_train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            \n",
    "            inputs, labels = data\n",
    "  \n",
    "  \n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            running_loss += loss_size.data.item() * batch_size\n",
    "            total_train_loss += loss_size.data.item() \n",
    "            \n",
    "            if i % n_batches//4 == 0 and i != 0: \n",
    "                print('Iteration {0}: \\n  \\\n",
    "                       Running Loss Is {1}'\n",
    "                      .format(i, running_loss/(100 * batch_size)))\n",
    "                running_loss = 0\n",
    "            \n",
    "        accuracy = 0 \n",
    "        with torch.no_grad():\n",
    "          for inputs, labels in test_loader: \n",
    "              if cuda:   \n",
    "                  inputs, labels = inputs.cuda(), labels.cuda()\n",
    "              val_outputs = net(inputs)\n",
    "              val_loss += loss(val_outputs, labels).data.item() \n",
    "              accuracy += (torch.max(val_outputs, 1).indices == labels).sum().item()\n",
    "\n",
    "        accuracy = accuracy/len(test_data)\n",
    "        train_loss = total_train_loss/(len(train_data)/batch_size)\n",
    "        val_loss = val_loss/(len(test_data)/batch_size)\n",
    "        print(val_loss)\n",
    "        print(\"Epoch {0}, \\n \\\n",
    "                Train Loss: {1} \\n \\\n",
    "                Validation Loss: {2} \\\n",
    "                Accuracy: {3}\".format(str(epoch), train_loss, val_loss, accuracy))\n",
    "        accuracy_list.append(accuracy)\n",
    "        validation_loss.append(val_loss)\n",
    "        training_loss.append(train_loss)\n",
    "        running_loss = 0\n",
    "\n",
    "                \n",
    "\n",
    "    return pd.DataFrame(data=[training_loss, validation_loss, accuracy_list],\n",
    "                        index=['train_loss', 'validation_loss', 'accuracy']).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_and_optimizer(architecture, learning_rate, optim_method, weight_decay):\n",
    "    \"\"\"\n",
    "        returns the loss and optimizer objects for pytorch to use in the model\n",
    "\n",
    "        arugment - \n",
    "            architecute: pytorch nn object\n",
    "            learning_rate: learning rate\n",
    "            optim_method: optimizer function\n",
    "        return\n",
    "            loss: loss object\n",
    "            optimizer: optimizer object\n",
    "    \"\"\"\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim_method(architecture.parameters(), lr = learning_rate, weight_decay=0)\n",
    "\n",
    "    return loss, optimizer\n",
    "\n",
    "def get_data_loader(dataset, batch_size):\n",
    "    \"\"\"\n",
    "        converts a dataset into a data loader object\n",
    "        argument - \n",
    "            dataset: pytorch dataset object\n",
    "            batch_size: batch_size\n",
    "        return \n",
    "            data loader object\n",
    "    \"\"\"\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0HklTinjsnIp"
   },
   "source": [
    "## Improved Cifar-10 Model\n",
    "\n",
    "* From the graph above, we can see that our vanilla convnet has overfit on the training data. To fix this, we will modify the preprocessing of our data and change our model architecture + optimization function. \n",
    "\n",
    "### Preprocessing changes\n",
    "1. Data augmentation functions to add more training data\n",
    "2. Normalization to scale the training data\n",
    "\n",
    "### Model architecture changes\n",
    "1. Batch Normalization\n",
    "  * Normalizes each channel of the batch, generally you do this before a point of nonlinearity. \n",
    "2. Dropout\n",
    "  * Randomly zeros out weights in training to keep certain features from getting too large, forcing the model to learn from different sets of features. I will be using this on the feed forward layers of the neural network. \n",
    "\n",
    "(Additionally, I utilized the nn.Sequential to make the model more architecture easier to read)\n",
    "\n",
    "### Notes\n",
    "For future improvement I can look into\n",
    "* Additional preprocessing functions\n",
    "  * tensorflow/pytorch have an extension CV library that can be used to improve performance.\n",
    "* Variable learning rate, such as utilizing the adam optimizer or a circular learning rate\n",
    "* Hyperparameter Tuning\n",
    "  * A simple way to do this is to use a randomized grid search + cross validation and a fair amount of compute. If I were looking to go further I could look into bayesian or bandit based tuning methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "Wak1TYuMCLKY",
    "outputId": "bb7801c8-d954-434d-b3cc-cf3f6e18d8ee"
   },
   "outputs": [],
   "source": [
    "class CNNNorm(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNNNorm, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 32, 3),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU(True),\n",
    "                                   nn.Conv2d(32, 64, 3),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(True))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, 3),\n",
    "                                   nn.BatchNorm2d(128),\n",
    "                                   nn.ReLU(True),\n",
    "                                   nn.Conv2d(128, 128, 3),\n",
    "                                   nn.BatchNorm2d(128),\n",
    "                                   nn.ReLU(True),\n",
    "                                   )\n",
    "          \n",
    "        self.conv3 = nn.Sequential(\n",
    "                                   nn.Conv2d(128, 256, 3),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(True),\n",
    "                                   nn.Conv2d(256, 256, 3),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(True),\n",
    "                                   )\n",
    "          \n",
    "        self.fc = nn.Sequential(\n",
    "                                nn.Linear(1024, 1024),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(1024, 1024),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(1024, 256),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(256, 10)\n",
    "                                )\n",
    "\n",
    "                                     \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2, padding=1)\n",
    "        \n",
    "   \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Don't need to run (f.softMax because BCEloss will run softmax while calculating)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "fti3iG58Duat",
    "outputId": "48c541a1-2d7b-456a-da00-89cbee0822ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: \n",
      "                         Running Loss Is 0.04858360528945923\n",
      "Iteration 2: \n",
      "                         Running Loss Is 0.021649088859558106\n",
      "Iteration 3: \n",
      "                         Running Loss Is 0.023021183013916015\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader('../data/cifar-10-batches-py/', [flip_horizontal])\n",
    "train, test, labels = data_loader.execute()\n",
    "\n",
    "model = CNNNorm()\n",
    "\n",
    "refined_convnet_params = {'batch_size': 20,\n",
    "                          'n_epochs': 50, \n",
    "                          'learning_rate': .001,\n",
    "                          'optim_method': optim.AdamW,\n",
    "                          'weight_decay': 5e-6\n",
    "                          }\n",
    "      \n",
    "losses = train_model(model, train, test, **refined_convnet_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Collab Life",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
