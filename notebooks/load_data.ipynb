{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os  \n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta  data_batch_2  data_batch_4  readme.html\n",
      "data_batch_1  data_batch_3  data_batch_5  test_batch\n"
     ]
    }
   ],
   "source": [
    "ls data/cifar-10-batches-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(input_dir):\n",
    "    \"\"\"Takes in an input training directory and returns \n",
    "    \n",
    "        train_data, train_labels\n",
    "        test_data, test_labels\"\"\"\n",
    "    training_files = map(lambda x: import_image(''.join([input_dir, '/', 'data_batch_', str(x)])), range(2, 5))\n",
    "    \n",
    "    train_data, train_labels = reduce(lambda x, y: (np.append(x[0], y[0], axis=0), np.append(x[1], y[1], axis=0)), training_files)\n",
    "    test_data, test_labels = import_image('data/cifar-10-batches-py/test_batch')\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_and_encode(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dic = pickle.load(fo, encoding='bytes')\n",
    "    return {str(k, 'utf-8'): v for k, v in dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_image(path):\n",
    "    raw_image_data = unpickle_and_encode(path)\n",
    "    \n",
    "    images, labels = raw_image_data['data'], raw_image_data['labels']\n",
    "    images = images.reshape((images.shape[0], 3, 32, 32)).transpose(0, 2, 3, 1) \n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img_object, label = None):\n",
    "    \n",
    "    plt.title(label)\n",
    "    plt.imshow(img_object)\n",
    "\n",
    "    \n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-np.min(x)) / (np.max(x)-np.min(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_files('data/cifar-10-batches-py')\n",
    "label_encodings = unpickle_and_encode('data/cifar-10-batches-py/batches.meta')['label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = normalize(train_data), normalize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VanillaCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VanillaCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VanillaCNN_With_Dropout(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VanillaCNN_With_Dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def loss_and_optimizer(net, learning_rate):\n",
    "    \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "    \n",
    "    return loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_loader(batch_size, train_data, train_labels):\n",
    "    \n",
    "    dataset = TensorDataset(torch.Tensor(train_data.transpose(0,3,1,2)),\n",
    "                            torch.LongTensor(train_labels))\n",
    "    if batch_size is None:\n",
    "        batch_size = len(train_data)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                        num_workers=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, batch_size, n_epochs, learning_rate, train_data, train_labels, test_data, test_labels):\n",
    "    training_loss, validation_loss, accuracy_list = [], [], []\n",
    "    train_loader = get_train_loader(batch_size, train_data, train_labels)\n",
    "    \n",
    "    test_loader = get_train_loader(None, test_data, test_labels)\n",
    "    n_batches = len(train_loader)\n",
    "    loss, optimizer = loss_and_optimizer(net, learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            \n",
    "            inputs, labels = data\n",
    "           # inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            running_loss += loss_size.data.item() * batch_size\n",
    "            total_train_loss += loss_size.data.item()\n",
    "\n",
    "                       \n",
    "        for inputs, labels in test_loader: \n",
    "            val_outputs = net(inputs)\n",
    "            val_loss = loss(val_outputs, labels).data.item()\n",
    "            accuracy = (torch.max(val_outputs, 1).indices == labels).sum().item()/test_data.shape[0]\n",
    "            print(val_loss)\n",
    "            print(\"Epoch {0}, \\n \\\n",
    "                   Train Loss: {1} \\n \\\n",
    "                   Validation Loss: {2} \\\n",
    "                   Accuracy: {3}\".format(str(epoch), total_train_loss/(train_data.shape[0]/batch_size), val_loss, accuracy))\n",
    "            \n",
    "            accuracy_list.append(accuracy)\n",
    "        validation_loss.append(val_loss)\n",
    "        training_loss.append(total_train_loss/(train_data.shape[0]/batch_size))\n",
    "        running_loss = 0\n",
    "\n",
    "                \n",
    "    \n",
    "    return training_loss, validation_loss, accuracy_list, \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302248954772949\n",
      "Epoch 0, \n",
      "                    Train Loss: 2.3039719701131185 \n",
      "                    Validation Loss: 2.302248954772949                    Accuracy: 0.1082\n",
      "2.2999823093414307\n",
      "Epoch 1, \n",
      "                    Train Loss: 2.3026705464680988 \n",
      "                    Validation Loss: 2.2999823093414307                    Accuracy: 0.1212\n",
      "2.2822935581207275\n",
      "Epoch 2, \n",
      "                    Train Loss: 2.296788776397705 \n",
      "                    Validation Loss: 2.2822935581207275                    Accuracy: 0.1312\n",
      "2.1713039875030518\n",
      "Epoch 3, \n",
      "                    Train Loss: 2.223897725168864 \n",
      "                    Validation Loss: 2.1713039875030518                    Accuracy: 0.1907\n",
      "2.00156307220459\n",
      "Epoch 4, \n",
      "                    Train Loss: 2.0711895495096844 \n",
      "                    Validation Loss: 2.00156307220459                    Accuracy: 0.2714\n",
      "1.937245488166809\n",
      "Epoch 5, \n",
      "                    Train Loss: 1.9567533711751302 \n",
      "                    Validation Loss: 1.937245488166809                    Accuracy: 0.2975\n",
      "1.936347246170044\n",
      "Epoch 6, \n",
      "                    Train Loss: 1.86161002591451 \n",
      "                    Validation Loss: 1.936347246170044                    Accuracy: 0.3047\n",
      "1.7541261911392212\n",
      "Epoch 7, \n",
      "                    Train Loss: 1.7768294144948324 \n",
      "                    Validation Loss: 1.7541261911392212                    Accuracy: 0.3621\n",
      "1.8007347583770752\n",
      "Epoch 8, \n",
      "                    Train Loss: 1.7187280202229818 \n",
      "                    Validation Loss: 1.8007347583770752                    Accuracy: 0.3561\n",
      "1.7262451648712158\n",
      "Epoch 9, \n",
      "                    Train Loss: 1.6732707328796386 \n",
      "                    Validation Loss: 1.7262451648712158                    Accuracy: 0.3856\n",
      "1.6530872583389282\n",
      "Epoch 10, \n",
      "                    Train Loss: 1.6416753557840984 \n",
      "                    Validation Loss: 1.6530872583389282                    Accuracy: 0.3953\n",
      "1.7049654722213745\n",
      "Epoch 11, \n",
      "                    Train Loss: 1.611013057200114 \n",
      "                    Validation Loss: 1.7049654722213745                    Accuracy: 0.3896\n",
      "1.691727876663208\n",
      "Epoch 12, \n",
      "                    Train Loss: 1.5831054388682047 \n",
      "                    Validation Loss: 1.691727876663208                    Accuracy: 0.395\n",
      "1.5830293893814087\n",
      "Epoch 13, \n",
      "                    Train Loss: 1.560483363087972 \n",
      "                    Validation Loss: 1.5830293893814087                    Accuracy: 0.4254\n",
      "1.5541712045669556\n",
      "Epoch 14, \n",
      "                    Train Loss: 1.539952229309082 \n",
      "                    Validation Loss: 1.5541712045669556                    Accuracy: 0.4331\n",
      "1.6028600931167603\n",
      "Epoch 15, \n",
      "                    Train Loss: 1.5266753698984783 \n",
      "                    Validation Loss: 1.6028600931167603                    Accuracy: 0.4301\n",
      "1.6076555252075195\n",
      "Epoch 16, \n",
      "                    Train Loss: 1.5086833670298259 \n",
      "                    Validation Loss: 1.6076555252075195                    Accuracy: 0.4259\n",
      "1.6100847721099854\n",
      "Epoch 17, \n",
      "                    Train Loss: 1.4936107113520305 \n",
      "                    Validation Loss: 1.6100847721099854                    Accuracy: 0.4291\n",
      "1.6354817152023315\n",
      "Epoch 18, \n",
      "                    Train Loss: 1.4829366470336913 \n",
      "                    Validation Loss: 1.6354817152023315                    Accuracy: 0.4237\n",
      "1.5478692054748535\n",
      "Epoch 19, \n",
      "                    Train Loss: 1.466105699666341 \n",
      "                    Validation Loss: 1.5478692054748535                    Accuracy: 0.4484\n",
      "1.5549900531768799\n",
      "Epoch 20, \n",
      "                    Train Loss: 1.4534928460439047 \n",
      "                    Validation Loss: 1.5549900531768799                    Accuracy: 0.452\n",
      "1.5470130443572998\n",
      "Epoch 21, \n",
      "                    Train Loss: 1.4430971249898275 \n",
      "                    Validation Loss: 1.5470130443572998                    Accuracy: 0.4523\n",
      "1.531307339668274\n",
      "Epoch 22, \n",
      "                    Train Loss: 1.4341110857009887 \n",
      "                    Validation Loss: 1.531307339668274                    Accuracy: 0.4555\n",
      "1.5590914487838745\n",
      "Epoch 23, \n",
      "                    Train Loss: 1.4238570467631022 \n",
      "                    Validation Loss: 1.5590914487838745                    Accuracy: 0.4519\n",
      "1.5831269025802612\n",
      "Epoch 24, \n",
      "                    Train Loss: 1.410213713200887 \n",
      "                    Validation Loss: 1.5831269025802612                    Accuracy: 0.4462\n",
      "1.4756624698638916\n",
      "Epoch 25, \n",
      "                    Train Loss: 1.404470582707723 \n",
      "                    Validation Loss: 1.4756624698638916                    Accuracy: 0.4728\n",
      "1.4715685844421387\n",
      "Epoch 26, \n",
      "                    Train Loss: 1.390677096303304 \n",
      "                    Validation Loss: 1.4715685844421387                    Accuracy: 0.479\n",
      "1.5359256267547607\n",
      "Epoch 27, \n",
      "                    Train Loss: 1.3846363701502482 \n",
      "                    Validation Loss: 1.5359256267547607                    Accuracy: 0.4581\n",
      "1.4471491575241089\n",
      "Epoch 28, \n",
      "                    Train Loss: 1.3741524399439493 \n",
      "                    Validation Loss: 1.4471491575241089                    Accuracy: 0.484\n",
      "1.4511276483535767\n",
      "Epoch 29, \n",
      "                    Train Loss: 1.3637026808420818 \n",
      "                    Validation Loss: 1.4511276483535767                    Accuracy: 0.4789\n",
      "1.4781303405761719\n",
      "Epoch 30, \n",
      "                    Train Loss: 1.3536584034601846 \n",
      "                    Validation Loss: 1.4781303405761719                    Accuracy: 0.4734\n",
      "1.5661392211914062\n",
      "Epoch 31, \n",
      "                    Train Loss: 1.3507211575826008 \n",
      "                    Validation Loss: 1.5661392211914062                    Accuracy: 0.4507\n",
      "1.4318206310272217\n",
      "Epoch 32, \n",
      "                    Train Loss: 1.3371359596252441 \n",
      "                    Validation Loss: 1.4318206310272217                    Accuracy: 0.4873\n",
      "1.4645904302597046\n",
      "Epoch 33, \n",
      "                    Train Loss: 1.3301707000096639 \n",
      "                    Validation Loss: 1.4645904302597046                    Accuracy: 0.4795\n",
      "1.3985856771469116\n",
      "Epoch 34, \n",
      "                    Train Loss: 1.32738922773997 \n",
      "                    Validation Loss: 1.3985856771469116                    Accuracy: 0.4949\n",
      "1.4169530868530273\n",
      "Epoch 35, \n",
      "                    Train Loss: 1.3167175980885824 \n",
      "                    Validation Loss: 1.4169530868530273                    Accuracy: 0.4941\n",
      "1.4053517580032349\n",
      "Epoch 36, \n",
      "                    Train Loss: 1.3097713956197103 \n",
      "                    Validation Loss: 1.4053517580032349                    Accuracy: 0.496\n",
      "1.4390915632247925\n",
      "Epoch 37, \n",
      "                    Train Loss: 1.2990286632537842 \n",
      "                    Validation Loss: 1.4390915632247925                    Accuracy: 0.4999\n",
      "1.4298402070999146\n",
      "Epoch 38, \n",
      "                    Train Loss: 1.2892430685679117 \n",
      "                    Validation Loss: 1.4298402070999146                    Accuracy: 0.4929\n",
      "1.4394893646240234\n",
      "Epoch 39, \n",
      "                    Train Loss: 1.2851403162002564 \n",
      "                    Validation Loss: 1.4394893646240234                    Accuracy: 0.4873\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4694e03594ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_vanilla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVanillaCNN_With_Dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_vanilla\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-5f67160c3abc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, batch_size, n_epochs, learning_rate, train_data, train_labels, test_data, test_labels)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "new_vanilla = VanillaCNN_With_Dropout()\n",
    "losses = train_model(new_vanilla, 32, 30, .01, train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fff3677caec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=losses, index=['train_loss','validation_loss', 'accuracy']).T\n",
    "df[['train_loss', 'validation_loss']].plot(kind='line')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifar-10-batches-py\u001b[0m/  \u001b[01;31minput_data.tar.gz\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-406417c8157b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "test_loader = get_train_loader(None, test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = new_vanilla(torch.Tensor(test_data.transpose(0,3,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 32, 32, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4154"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.max(res, 1).indices == torch.Tensor(test_labels)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
